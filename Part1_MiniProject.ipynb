{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "$$\n",
    "\n",
    "# Part 1: Mini-Project\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "#### Creating the Dataset\n",
    "###### Downloading the data\n",
    "###### Data Directory Preprocessing\n",
    "###### Datasets creation with the Roboflow API\n",
    "\n",
    "#### Creating the Model\n",
    "###### YOLOv8\n",
    "###### Model Architecture\n",
    "###### Loss function\n",
    "###### Optimization\n",
    "\n",
    "#### Evaluating the Model\n",
    "###### Predicting on the test set\n",
    "###### COCO evaluation\n",
    "\n",
    "#### Results and Discussion\n",
    "###### Training and Validation results\n",
    "###### yolo test results\n",
    "###### coco evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating the Datasets\n",
    "\n",
    "### Downloading the data\n",
    "As instructed for the project, we've used the stable version of TACO dataset which consists of 1500 images and 4787 annotations.\n",
    "The images were downloaded from the https://github.com/pedropro/TACO repository.\n",
    "The annotations were downloaded from https://github.com/wimlds-trojmiasto/detect-waste/tree/main/annotations repository.\n",
    "We've used the annotations_train.json and annotations_test.json files, including 7 detect-waste categories for the object detection task:\n",
    "- bio\n",
    "- glass\n",
    "- metals_and_plastic\n",
    "- non_recyclable\n",
    "- other\n",
    "- paper\n",
    "- unknown\n",
    "\n",
    "### Data Directory Preprocessing\n",
    "We used the Roboflow API to create our datasets. In order to use the Roboflow API we've implemnted a python script that edits the data directory in two steps (the script is ran in the next cell).\n",
    "1. Flattening the directory: The original structure of the directory had 15 sub-directories (batches 1-15) each containing ~100 images, each image named as a number in [1, ~100]. We flattend the subdirectories to one directory, and changed the images names, adjusting the corresponding images names in the annotations files.\n",
    "2. Splitting the directory into two: Test_dir and Train_dir, based on the images partition in the annotation files.\n",
    "\n",
    "### Datasets creation with the Roboflow API\n",
    "Once the data directory was prepared, we've uploaded it to Roboflow and created two seperated datasets.\n",
    "1. A training set with 1182 images (~79% of the data). We split this dataset to two subsets: Train set (1062 images, 90%) and a validation subset (118 images, 10%).\n",
    "2. A test set with 317 images. (~21% of the data).\n",
    "\n",
    "With the roboflow API we've processed the images with the following tools:\n",
    "1. resized all training images to 640x640.\n",
    "2. applied auto-orientation to correct mismatchs between annotations and images.\n",
    "\n",
    "Then We created the dataset, which can be downloaded using the code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from roboflow import Roboflow\n",
    "# from ultralytics import YOLO\n",
    "# import project.model_training as mt\n",
    "\n",
    "# preprocess data directory to fit roboflow\n",
    "# the script is commented as the data location in the server may vary. \n",
    "# [it is said to be in `datasets/TACO-master` but we could not find it there]  \n",
    "\n",
    "# %run project/preprocess_imgs.py\n",
    "\n",
    "# upload the processed data to roboflow as explained above\n",
    "\n",
    "# download the datasets\n",
    "# train_set, test_set = mt.load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating the Model\n",
    "### YOLOv8\n",
    "We chose to approach the task by custom training the YOLOv8 model. This model is regarded as one of the leading models in image classification, detection and segmentation. To achive best results, we've used the largest, most accurate version of the model (YOLOv8x). We trained our model for a 100 epochs, taking into considaration 3 main factors:\n",
    "1. Maximizing validation result during the training process\n",
    "2. Avoiding overfitting to the training set\n",
    "3. least important factor, but still realevent: Costâ€“benefit analysis for time and resources consumption during training the model [i.e: with more time and resources, it is possible to run more training sessions, perform cross-validation etc to reach better results]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing and training the model.\n",
    "# This code block is commented so that the notebook wont preform training.\n",
    "# To re-create our the training process, uncomment and run the next line\n",
    "\n",
    "# model, train_res = mt.set_model(train_set, 'yolov8x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To avoid training, load our trained model:\n",
    "# model = YOLO(\"runs/detect/train27/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Architecture:\n",
    "YOLO V8 consists of two main components. A backbone and a head. The backbone is a series of convolutional networks and course to fine (C2f) layers. The backbone creates features which are then passed to the head for detection using the models loss function. A diagram by [RangeKing](https://github.com/RangeKing) of the model can be seen here.\n",
    "\n",
    "<div>\n",
    "<img src=\"imgs/yolov8_architecture_diagram.jpeg\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "Sublayers are included in the diagram and it illustrates each well.\n",
    "\n",
    "The architecture utilizes bottlenecks and a pyramidal structure for the architecture. One pyramidal concept is the spatial pyramid pooling layers (SPP/SPPF).\n",
    "\n",
    "Some changes in this version of YOLO include;  \n",
    "\n",
    "    - Not using anchor boxes for detection which increased speed.\n",
    "    \n",
    "    - A new backbone consisting of new convolutional building block and new C2f layers which have additional residual connections.\n",
    "    \n",
    "    - And new loss functions\n",
    "    \n",
    "The full model can be seen here on the [YOLOv8 repo](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/v8/yolov8.yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function:\n",
    "\n",
    "The model uses a loss function that combines several elements to measure the total loss.\n",
    "\n",
    "- The first part is a Bbox Loss. The bbox loss returns two seperate loss values. The Bbox loss holds a componenet of the total loss that measures and evaluates the loss of the bounding boxes generated by the model.  \n",
    "\n",
    "1. IoU Loss: Which is a standard intersection over union loss. Calculated by using an external bbox_iou method.\n",
    "\n",
    "2. DFL Loss: Which is a distributional focal loss function. As proposed in this [paper](https://ieeexplore.ieee.org/document/9792391). In short this is a loss function that also measures the quality of box locations but does so using distribution based methods. \n",
    "\n",
    "Below is the code of the Bbox loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class BboxLoss(nn.Module):\n",
    "\n",
    "#     def __init__(self, reg_max, use_dfl=False):\n",
    "#         super().__init__()\n",
    "#         self.reg_max = reg_max\n",
    "#         self.use_dfl = use_dfl\n",
    "\n",
    "#     def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n",
    "#         # IoU loss\n",
    "#         weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
    "#         iou = bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask], xywh=False, CIoU=True)\n",
    "#         loss_iou = ((1.0 - iou) * weight).sum() / target_scores_sum\n",
    "\n",
    "#         # DFL loss\n",
    "#         if self.use_dfl:\n",
    "#             target_ltrb = bbox2dist(anchor_points, target_bboxes, self.reg_max)\n",
    "#             loss_dfl = self._df_loss(pred_dist[fg_mask].view(-1, self.reg_max + 1), target_ltrb[fg_mask]) * weight\n",
    "#             loss_dfl = loss_dfl.sum() / target_scores_sum\n",
    "#         else:\n",
    "#             loss_dfl = torch.tensor(0.0).to(pred_dist.device)\n",
    "\n",
    "#         return loss_iou, loss_dfl\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _df_loss(pred_dist, target):\n",
    "#         # Return sum of left and right DFL losses\n",
    "#         # Distribution Focal Loss (DFL) proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "#         tl = target.long()  # target left\n",
    "#         tr = tl + 1  # target right\n",
    "#         wl = tr - target  # weight left\n",
    "#         wr = 1 - wl  # weight right\n",
    "#         return (F.cross_entropy(pred_dist, tl.view(-1), reduction='none').view(tl.shape) * wl +\n",
    "#                 F.cross_entropy(pred_dist, tr.view(-1), reduction='none').view(tl.shape) * wr).mean(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second part is a Varifocal loss which gives a classification loss component to the total loss. It is defined in this [paper](https://arxiv.org/pdf/2008.13367.pdf) as:  \n",
    "\n",
    "<div>\n",
    "<img src=\"imgs/VFL3.png\" width=\"500\"/>\n",
    "</div>\n",
    "<div>\n",
    "<img src=\"imgs/VFL2.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Which is a take on binary cross entropy and is further explained in detail in the paper. In general focal losses help classify when we have imbalanced classes. Where some examples are easily classified and others are more difficult, the loss then focuses more on the challenging examples. In general this is a strong classification loss function.\n",
    "\n",
    "We can see that the code of the loss function also includes an existing binary cross entropy method: binary_cross_entropy_with_logits\n",
    "\n",
    "Which from its documentation is a combination of binary cross entropy with a sigmoid layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class VarifocalLoss(nn.Module):\n",
    "#     # Varifocal loss by Zhang et al. https://arxiv.org/abs/2008.13367\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, pred_score, gt_score, label, alpha=0.75, gamma=2.0):\n",
    "#         weight = alpha * pred_score.sigmoid().pow(gamma) * (1 - label) + gt_score * label\n",
    "#         with torch.cuda.amp.autocast(enabled=False):\n",
    "#             loss = (F.binary_cross_entropy_with_logits(pred_score.float(), gt_score.float(), reduction='none') *\n",
    "#                     weight).sum()\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization: \n",
    "\n",
    "The YOLOv8 model uses a default optimizer of ADAM.\n",
    "ADAM is an extented version of stochastic gradient decent with momentum that only uses first order gradients andÂ \n",
    "is based on adaptive estimates of lower-order moments. Empirical results show that adam produces good results in comparison to other\n",
    "optimizer algorithms and is well suited for large data.\n",
    "the default hyper parameters in the model are: Learning rate=0.001, Momentum=0.9, Decay=1e-5\n",
    "\n",
    "We choose to use this optimizer relying on the fact that ADAM is a SOTA optimization algorithim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "### Predicting on the test set\n",
    "After we trained the model, we used the YOLO.val() method to predict on our test set.\n",
    "The method performs object detection on our unseen test set images.\n",
    "To deeper our model evaluation, we've performed another analysis using the cocotools. \n",
    "To perform the evaluation, we compared the ground truth annotation file and the detected annotations for the test dataset.\n",
    "The detected annotation file is created by the VAL() function, and processed by our next script to fit the cocoEVAL() comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# edit the test_set to fit evaluations\n",
    "# %run project/edit_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict with the model on the test set for evaluation\n",
    "# test_res = mt.evaluate_model(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform the COCO evaluation\n",
    "# %run project/cocoEval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results and dicussion\n",
    "### Training and Validation results\n",
    "During the training process, the model validates its performence after each epoch on the validation subset.\n",
    "- Reminder & Clarification: Our validation subset is part of the training Dataset but the model does not train on it explicitly. This means that the validation subset does not affect the weights directly. However, as we use those results to manualy tune the model's version, size and hyperparameters, the validation subset is *not* used for the test results (which we'll discuss later on). \n",
    "\n",
    "In the following graph, we can see the model's performence as a function of the training epochs.\n",
    "<div>   \n",
    "<img src=\"imgs/results_train.png\" width=\"800\"/>  \n",
    "</div>\n",
    "\n",
    "* Few important distinctions:\n",
    "1. As expected, we can see that during the training process the loss values (box, cls, dfl) decrease and the precision values increase for both training and validation set.\n",
    "2. We see that the graphs did not reach a plateau yet, indicating we might still be able to improve the model performence. After severall training processes, we've noticed that we can still improve the model's precision on the training set, but this will cause overfitting effect for some of the categories. Therfore we limited the model epochs. \n",
    "\n",
    "In the following figure, we can observe the different predictions disribution broken into categories.\n",
    "<div>   \n",
    "<img src=\"imgs/matrix_train.png\" width=\"800\"/>  \n",
    "</div>\n",
    "\n",
    "* We can see that there are two categories that are \"overpredicted\"(False Positive - FP): background and metals_and_plastic.\n",
    "We can lower the background FP value by decreasing the confidence threshold for the detection. However, this lowers the overall precision as it increases the FN values, and specifically increases the metals_and_plastic FP value.\n",
    "\n",
    "* Another distinction is that the model rarely predicts the \"other\" category. We assume that this happens due to the fact that as opposed to the rest of the cattegories, \"other\" has no dintinct definition. therfore the model cant find unique patterns (weights) to detect it.\n",
    "\n",
    "* The 'bio' column is empty as there are no bio labels in the validation set (generally, there are very few bio labels in the TACO dataset, hence we expect that the model wont detect them well or wont detect them at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO test results\n",
    "\n",
    "<div>   \n",
    "<img src=\"imgs/matrix_test_3.png\" width=\"800\"/>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the training results we can again notice the over-prediction of background and metals_and_plastics which is not surprising having been a difficulty of the model during training.\n",
    "\n",
    "There were no major changes in comparison to the training. However we can notice some added confusion between glass and non-recyclables and unknown and paper. The changes are not major and we can still observe similar results.\n",
    "\n",
    " When taking into account the large variability of the data set and the fact that we trained a large model and likely slightly overfitted the data to some inevitable extent the results are reasonable. It is worth noting that in general the entire YOLOv8 model barely reaches over 50 mAP on the COCO 2017 image dataset. Considering that the dataset is substantially smaller and that we had less time and resources to train the model on it, we think the results are decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### COCO evaluation results\n",
    "\n",
    "<div>   \n",
    "<img src=\"imgs/coco_eval.png\" width=\"600\"/>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see that the results are similar to the results produced in the training set. One thing worth noting is that with the final Average recall with IoU=0.5:095 and large area we reach an Average recall of 0.13 which is decent recall value when taking into consideration the limitations of the task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "db2aeb4a2d20d8f60ca10a95a693e4fbc8d4e86da5b1f77d6fc445990905a603"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "$$\n",
    "\n",
    "# Part 1: Mini-Project\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You should implement the code which displays your results in this notebook, and add any additional code files for your implementation in the `project/` directory. You can import these files here, as we do for the homeworks.\n",
    "- Running this notebook should not perform any training - load your results from some output files and display them here. The notebook must be runnable from start to end without errors.\n",
    "- You must include a detailed write-up (in the notebook) of what you implemented and how. \n",
    "- Explain the structure of your code and how to run it to reproduce your results.\n",
    "- Explicitly state any external code you used, including built-in pytorch models and code from the course tutorials/homework.\n",
    "- Analyze your numerical results, explaining **why** you got these results (not just specifying the results).\n",
    "- Where relevant, place all results in a table or display them using a graph.\n",
    "- Before submitting, make sure all files which are required to run this notebook are included in the generated submission zip.\n",
    "- Try to keep the submission file size under 10MB. Do not include model checkpoint files, dataset files, or any other non-essentials files. Instead include your results as images/text files/pickles/etc, and load them for display in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection on TACO dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TACO is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches.\n",
    "\n",
    "<center><img src=\"imgs/taco.png\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can read more about the dataset here: https://github.com/pedropro/TACO\n",
    "\n",
    "and can explore the data distribution and how to load it from here: https://github.com/pedropro/TACO/blob/master/demo.ipynb\n",
    "\n",
    "\n",
    "The stable version of the dataset that contain 1500 images and 4787 annotations exist in `datasets/TACO-master`\n",
    "You do not need to download the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project goals:\n",
    "\n",
    "* You need to perform Object Detection task, over 7 of the dataset.\n",
    "* The annotation for object detection can be downloaded from here: https://github.com/wimlds-trojmiasto/detect-waste/tree/main/annotations.\n",
    "* The data and annotation format is like the COCOAPI: https://github.com/cocodataset/cocoapi (you can find a notebook of how to perform evalutation using it here: https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb)\n",
    "(you need to install it..)\n",
    "* if you need a beginner guild for OD in COCOAPI, you can read and watch this link: https://www.neuralception.com/cocodatasetapi/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do i need to do?\n",
    "\n",
    "* **Everything is in the game!** as long as your model does not require more then 8 GB of memory and you follow the Guidelines above.\n",
    "\n",
    "\n",
    "### What does it mean?\n",
    "* you can use data augmentation, rather take what's implemented in the directory or use external libraries such as https://albumentations.ai/ (notice that when you create your own augmentations you need to change the annotation as well)\n",
    "* you can use more data if you find it useful (for examples, reviwew https://github.com/AgaMiko/waste-datasets-review)\n",
    "\n",
    "\n",
    "### What model can i use?\n",
    "* Whatever you want!\n",
    "you can review good models for the coco-OD task as a referance:\n",
    "SOTA: https://paperswithcode.com/sota/object-detection-on-coco\n",
    "Real-Time: https://paperswithcode.com/sota/real-time-object-detection-on-coco\n",
    "Or you can use older models like YOLO-V3 or Faster-RCNN\n",
    "* As long as you have a reason (complexity, speed, preformence), you are golden.\n",
    "\n",
    "### Tips for a good grade:\n",
    "* start as simple as possible. dealing with APIs are not the easiest for the first time and i predict that this would be your main issue. only when you have a running model that learn, you can add learning tricks.\n",
    "* use the visualization of a notebook, as we did over the course, check that your input actually fitting the model, the output is the desired size and so on.\n",
    "* It is recommanded to change the images to a fixed size, like shown in here :https://github.com/pedropro/TACO/blob/master/detector/inspect_data.ipynb\n",
    "* Please adress the architecture and your loss function/s in this notebook. if you decided to add some loss component like the Focal loss for instance, try to show the results before and after using it.\n",
    "* Plot your losses in this notebook, any evaluation metric can be shown as a function of time and possibe to analize per class.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep project\n",
    "\n",
    "structure of this file:\n",
    "- bla\n",
    "-\n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating the Datasets\n",
    "\n",
    "### Downloading the data\n",
    "As instructed for the project, we've used the stable version of TACO dataset which consists of 1500 images and 4787 annotations.\n",
    "The images were downloaded from the https://github.com/pedropro/TACO repository.\n",
    "The annotations were downloaded from https://github.com/wimlds-trojmiasto/detect-waste/tree/main/annotations repository.\n",
    "We've used the annotations_train.json and annotations_test.json files, including 7 detect-waste categories for the object detection:\n",
    "- bio\n",
    "- glass\n",
    "- metals_and_plastic\n",
    "- non_recyclable\n",
    "- other\n",
    "- paper\n",
    "- unknown\n",
    "\n",
    "### Data Directory Preprocessing\n",
    "We used the Roboflow API to create our datasets. In order to use the Roboflow API we've implemnted a python script that edits the data directory in two steps (the script is ran in the next cell).\n",
    "1. Flattening the directory: The original structure of the directory had 15 sub-directories (batches 1-15) each containing ~100 images, each image named as a number in [1, ~100]. We flattend the subdirectories to one directory, and changed the images names, adjusting the corresponding images names in the annotations files.\n",
    "2. Splitting the directory into two: Test_dir and Train_dir, based on the images partition in the annotation files.\n",
    "\n",
    "### Datasets creation with the Roboflow API\n",
    "Once the data directory was prepared, we've uploaded it to Roboflow and created two seperated datasets.\n",
    "1. A training set with 1182 images (~79% of the data). We split this dataset to two subsets: Train set (1062 images, 90%) and a validation subset (118 images, 10%).\n",
    "2. A test set with 317 images. (~21% of the data).\n",
    "\n",
    "With the roboflow API we've processed the images with the following tools:\n",
    "1. resized all training images to 640x640.\n",
    "2. applied auto-orientation to correct mismatchs between annotations and images.\n",
    "\n",
    "Then We created the dataset, which can be downloaded using the code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Deep_project/project/preprocess_imgs.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# change directory structure and images names before uploading to roboflow\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# --1-- all images are added to the same directory with unique names\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# annotaion files were adjusted to the new naming\u001b[39;00m\n\u001b[1;32m      7\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m batch):\n\u001b[1;32m     10\u001b[0m         old_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m img_name\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics<=8.0.20 is required but found version=8.0.59, to fix: `pip install ultralytics<=8.0.20`\n",
      "Downloading Dataset Version Zip in TACO_train_only-1 to yolov8: 8% [10641408 / 126861221] bytes"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproject/preprocess_imgs.py\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# download the datasets\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_set, test_set \u001b[38;5;241m=\u001b[39m \u001b[43mmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Deep_project/project/model_training.py:12\u001b[0m, in \u001b[0;36mload_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# load the train set\u001b[39;00m\n\u001b[1;32m     11\u001b[0m train_project \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mworkspace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtechnion-fl2u0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mproject(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaco_train_only\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_project\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# load the test set\u001b[39;00m\n\u001b[1;32m     14\u001b[0m test_project \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mworkspace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtechnion-fl2u0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mproject(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaco_test_set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/roboflow/core/version.py:219\u001b[0m, in \u001b[0;36mVersion.download\u001b[0;34m(self, model_format, location, overwrite)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m    217\u001b[0m             response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__download_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__extract_zip(location, model_format)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reformat_yaml(location, model_format)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/roboflow/core/version.py:633\u001b[0m, in \u001b[0;36mVersion.__download_zip\u001b[0;34m(self, link, location, format)\u001b[0m\n\u001b[1;32m    630\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     \u001b[43mwget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/roboflow.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbar_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when trying to download dataset @ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/wget.py:526\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(url, out, bar)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     binurl \u001b[38;5;241m=\u001b[39m url\n\u001b[0;32m--> 526\u001b[0m (tmpfile, headers) \u001b[38;5;241m=\u001b[39m \u001b[43mulib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinurl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmpfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m filename \u001b[38;5;241m=\u001b[39m detect_filename(url, out, headers)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outdir:\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/urllib/request.py:283\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    281\u001b[0m             blocknum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    282\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m reporthook:\n\u001b[0;32m--> 283\u001b[0m                 \u001b[43mreporthook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocknum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m read \u001b[38;5;241m<\u001b[39m size:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentTooShortError(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieval incomplete: got only \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;241m%\u001b[39m (read, size), result)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/wget.py:513\u001b[0m, in \u001b[0;36mdownload.<locals>.callback_charged\u001b[0;34m(blocks, block_size, total_size)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcallback_charged\u001b[39m(blocks, block_size, total_size):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# 'closure' to set bar drawing function in callback\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[43mcallback_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbar_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/wget.py:471\u001b[0m, in \u001b[0;36mcallback_progress\u001b[0;34m(blocks, block_size, total_size, bar_function)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     current_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(blocks\u001b[38;5;241m*\u001b[39mblock_size, total_size)\n\u001b[0;32m--> 471\u001b[0m progress \u001b[38;5;241m=\u001b[39m \u001b[43mbar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress:\n\u001b[1;32m    473\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m progress)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/roboflow/core/version.py:630\u001b[0m, in \u001b[0;36mVersion.__download_zip.<locals>.bar_progress\u001b[0;34m(current, total, width)\u001b[0m\n\u001b[1;32m    622\u001b[0m progress_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset Version Zip in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;241m+\u001b[39m location\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m] bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (current \u001b[38;5;241m/\u001b[39m total \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, current, total)\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    629\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m progress_message)\n\u001b[0;32m--> 630\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/ipykernel/iostream.py:486\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# wait for flush to actually get through, if we can.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m evt \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mEvent()\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_timeout):\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/ipykernel/iostream.py:210\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     f()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/zmq/sugar/socket.py:620\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    613\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[1;32m    614\u001b[0m             data,\n\u001b[1;32m    615\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[1;32m    616\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    617\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[1;32m    618\u001b[0m         )\n\u001b[1;32m    619\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:746\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:793\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/zmq/backend/cython/checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "import project.model_training as mt\n",
    "\n",
    "# preprocess data directory to fit roboflow\n",
    "# the script is commented as the data location in the server may vary. \n",
    "# [it is said to be in `datasets/TACO-master` but we could not find it there]  \n",
    "# %run project/preprocess_imgs.py\n",
    "\n",
    "# download the datasets\n",
    "train_set, test_set = mt.load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating the Model\n",
    "### YOLOv8\n",
    "We chose to approach the task by custom training the YOLOv8 model. This model is regarded as one of the leading models in image classification, detection and segmentation. To achive best results, we've used the largest, most accurate version of the model (YOLOv8x). We trained our model for a 100 epochs, taking into considaration 3 main factors:\n",
    "1. Maximizing validation result during the training process\n",
    "2. Avoiding overfitting to the training set\n",
    "3. Costâ€“benefit analysis for time and resources consumption during training the model [i.e: with more time and resources, it is possible to run more training sessions, perform cross-validation etc to reach better results]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing and training the model.\n",
    "# This code block is commented so that the notebook wont preform training.\n",
    "# To re-create our the training process, uncomment and run the next line\n",
    "\n",
    "# model, train_res = mt.set_model(train_set, 'yolov8x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid training, load our trained model:\n",
    "model = YOLO(\"runs/detect/train27/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Architecture:\n",
    "YOLO V8 consists of two main components. A backbone and a head. The backbone is a series of convolutional networks and course to fine (C2f) layers. The backbone creates features which are then passed to the head for detection using the models loss function. A diagram by [RangeKing](https://github.com/RangeKing) of the model can be seen here.\n",
    "\n",
    "<div>\n",
    "<img src=\"imgs/yolov8_architecture_diagram.jpeg\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "Sublayers are included in the diagram and it illustrates each well.\n",
    "\n",
    "The architecture utilizes bottlenecks and a pyramidal structure for the architecture. One pyramidal concept is the spatial pyramid pooling layers (SPP/SPPF).\n",
    "\n",
    "Some changes in this version of YOLO include;  \n",
    "    - Not using anchor boxes for detection which increased speed.\n",
    "    - A new backbone consisting of new convolutional building block and new C2f layers which have additional residual connections.\n",
    "    - And new loss functions\n",
    "    \n",
    "The full model can bee seen here on the [YOLOv8 repo](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/v8/yolov8.yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function:\n",
    "\n",
    "The model uses a loss function that combines several elements to measure the total loss.\n",
    "\n",
    "- The first part is a Bbox Loss. The bbox loss returns two seperate loss values. The Bbox loss holds a componenet of the total loss that measures and evaluates the loss of the bounding boxes generated by the model.  \n",
    "\n",
    "1. IoU Loss: Which is a standard intersection over union loss. Calculated by using an external bbox_iou method.\n",
    "\n",
    "2. DFL Loss: Which is a distributional focal loss function. As proposed in this [paper](https://ieeexplore.ieee.org/document/9792391). In short this is a loss function that also measures the quality of box locations but does so using distribution based methods. \n",
    "\n",
    "Below is the code of the Bbox loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BboxLoss(nn.Module):\n",
    "\n",
    "#     def __init__(self, reg_max, use_dfl=False):\n",
    "#         super().__init__()\n",
    "#         self.reg_max = reg_max\n",
    "#         self.use_dfl = use_dfl\n",
    "\n",
    "#     def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n",
    "#         # IoU loss\n",
    "#         weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
    "#         iou = bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask], xywh=False, CIoU=True)\n",
    "#         loss_iou = ((1.0 - iou) * weight).sum() / target_scores_sum\n",
    "\n",
    "#         # DFL loss\n",
    "#         if self.use_dfl:\n",
    "#             target_ltrb = bbox2dist(anchor_points, target_bboxes, self.reg_max)\n",
    "#             loss_dfl = self._df_loss(pred_dist[fg_mask].view(-1, self.reg_max + 1), target_ltrb[fg_mask]) * weight\n",
    "#             loss_dfl = loss_dfl.sum() / target_scores_sum\n",
    "#         else:\n",
    "#             loss_dfl = torch.tensor(0.0).to(pred_dist.device)\n",
    "\n",
    "#         return loss_iou, loss_dfl\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _df_loss(pred_dist, target):\n",
    "#         # Return sum of left and right DFL losses\n",
    "#         # Distribution Focal Loss (DFL) proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "#         tl = target.long()  # target left\n",
    "#         tr = tl + 1  # target right\n",
    "#         wl = tr - target  # weight left\n",
    "#         wr = 1 - wl  # weight right\n",
    "#         return (F.cross_entropy(pred_dist, tl.view(-1), reduction='none').view(tl.shape) * wl +\n",
    "#                 F.cross_entropy(pred_dist, tr.view(-1), reduction='none').view(tl.shape) * wr).mean(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second part is a Varifocal loss which gives a classification loss component to the total loss. It is defined this paper in this [paper](https://arxiv.org/pdf/2008.13367.pdf) as:  \n",
    "\n",
    "<div>\n",
    "<img src=\"imgs/VFL3.png\" width=\"500\"/>\n",
    "</div>\n",
    "<div>\n",
    "<img src=\"imgs/VFL2.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Which is a take on binary cross entropy and is further explained in detail in the paper. In general focal losses help classify when we have imbalanced classes. Where some examples are easily classified and others are more difficult, the loss then focuses more on the challenging examples. In general this is a strong classification loss function.\n",
    "\n",
    "We can see that the code of the loss function also includes an existing binary cross entropy method: binary_cross_entropy_with_logits\n",
    "\n",
    "Which from its documentation is a combination of binary cross entropy with a sigmoid layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class VarifocalLoss(nn.Module):\n",
    "#     # Varifocal loss by Zhang et al. https://arxiv.org/abs/2008.13367\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, pred_score, gt_score, label, alpha=0.75, gamma=2.0):\n",
    "#         weight = alpha * pred_score.sigmoid().pow(gamma) * (1 - label) + gt_score * label\n",
    "#         with torch.cuda.amp.autocast(enabled=False):\n",
    "#             loss = (F.binary_cross_entropy_with_logits(pred_score.float(), gt_score.float(), reduction='none') *\n",
    "#                     weight).sum()\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization: \n",
    "\n",
    "The YOLOv8 model uses a default optimizer of ADAM with the following default hyper parameters.\n",
    "\n",
    "Learning rate=0.001, Momentum=0.9, Decay=1e-5\n",
    "\n",
    "We choose to use this optimizer relying on the fact that ADAM is a SOTA optimization algorithim and the model was designed around these hyperparams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Additional evaluation metrics?\n",
    "#### Accuracy:\n",
    "#### Results:\n",
    "#### Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "### Predicting on the test set\n",
    "After we trained the model, we used the YOLO.val() method to predict on our test set.\n",
    "The method performs object detection on our unseen test set images. After the detection, a confusion matrix is produced: \n",
    "MAP values:... \n",
    "graphs: ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.59 ðŸš€ Python-3.8.12 torch-1.10.1 CUDA:0 (NVIDIA GeForce RTX 2080, 7982MiB)\n",
      "Model summary (fused): 268 layers, 68130309 parameters, 0 gradients, 257.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/muradek/Deep_project/TACO_test_set-1/valid/labels... 317 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 317/317 [00:00<00:00, 511.57it\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/muradek/Deep_project/TACO_test_set-1/valid/labels.cache\n",
      "This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.37s/it]\n",
      "                   all        317        957     0.0425     0.0635     0.0267     0.0179\n",
      "                   bio        317         32          0          0    0.00116   0.000463\n",
      "                 glass        317        475      0.143     0.0316     0.0785      0.054\n",
      "    metals_and_plastic        317        262     0.0872      0.321     0.0601     0.0408\n",
      "        non_recyclable        317          1          0          0   0.000615   0.000553\n",
      "                 other        317         47          0          0          0          0\n",
      "                 paper        317        140     0.0247     0.0286     0.0201     0.0114\n",
      "Speed: 1.0ms preprocess, 31.6ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Saving runs/detect/val18/predictions.json...\n",
      "Results saved to \u001b[1mruns/detect/val18\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# predict with the model on the test set for evaluation\n",
    "test_res = mt.evaluate_model(test_set, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO evaluation\n",
    "To deeper our model evaluation, we've performed another analysis using the cocotools. \n",
    "To perform the evaluation, we compared the groung truth annotation file and the detected annotations for the dataset.\n",
    "The detected annotation file is created by the VAL() function, and processed by our next script to fit the cocoEVAL() comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.46s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.16s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.017\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.021\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.025\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.041\n"
     ]
    }
   ],
   "source": [
    "# perform the COCO evaluation\n",
    "%run project/pycocoEvalDemo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/muradek/Deep_project/project/annotations_test.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# edit the test_set to fit evaluations\n",
    "%run project/edit_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results and dicussion\n",
    "### Training and Validation results\n",
    "During the training process, the model validates its performence after each epoch on the validation subset.\n",
    "- Reminder & Clarification: Our validation subset is part of the training Dataset but the model does not train on it explicitly. This means that the validation subset does not affect the weights directly. However, as we use those results to mannualy tune the model's version, size and hyperparameters, the validation subset is *not* used for the test results (which we'll discuss later on). \n",
    "\n",
    "In the following graph, we can see the model's performence as a function of the training epochs.\n",
    "<div>   \n",
    "<img src=\"runs/detect/train27/results.png\" width=\"800\"/>  \n",
    "</div>\n",
    "Few important distinctions:\n",
    "1. As expected, we can see that during the training process the loss values (box, cls, dfl) decrease and the precision values increase for both training and validation set.\n",
    "2. We see that the values did not reach a flat line yet, inticatin we might still be able to improve the model performence. After severall training process, we've noticed that we can still improve the models precision on the training set, but this will cause overfitting effect for some of the categories. Therfore we limited the model epochs. \n",
    "\n",
    "In the following figure, we can observe the different predictions disribution broken into categories.\n",
    "<div>   \n",
    "<img src=\"runs/detect/train27/confusion_matrix.png\" width=\"600\"/>  \n",
    "</div>\n",
    "We can see that there are two categories that are \"overpredicted\"(False Positive - FP): background and metals_and_plastic.\n",
    "We can lower the the background FP by decreasing the confidence threshold for the detection. However, this leads to lower precision overall by increasing the FN, and specifically increases the metals_and_platic FP.\n",
    "\n",
    "Another distinction is that the model rarely predicts the \"other\" category. We assume that the reason is the fact that as opposed to the rest of the cattegories, \"other\" has no dintinct definition. therfore the model cant find uniqu patterns (wights) to detect it.\n",
    "\n",
    "### yolo test results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>   \n",
    "<img src=\"runs/detect/val18/confusion_matrix.png\" width=\"600\"/>  \n",
    "</div>\n",
    "### coco evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "db2aeb4a2d20d8f60ca10a95a693e4fbc8d4e86da5b1f77d6fc445990905a603"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
